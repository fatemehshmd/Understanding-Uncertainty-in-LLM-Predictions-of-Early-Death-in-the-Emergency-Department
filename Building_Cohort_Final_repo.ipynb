{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffdd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"ADMISSIONS.csv.gz\"\n",
    "df_adm = pd.read_csv(path, compression='gzip', header=0)\n",
    "df_adm.drop_duplicates(inplace=True)\n",
    "df_adm = df_adm.sort_values('ADMITTIME', ascending = True)\n",
    "# df_adm.dropna(subset=['DIAGNOSIS'],inplace=True)\n",
    "df_adm.dropna(subset=['HADM_ID','SUBJECT_ID'],inplace=True)\n",
    "df_adm = df_adm.reset_index().drop('index',axis=1)\n",
    "print(len(df_adm))\n",
    "print(df_adm.HADM_ID.nunique())\n",
    "df_adm.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm.ADMISSION_TYPE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_adm\n",
    "# Convert datetime columns to pandas datetime format\n",
    "df[\"ADMITTIME\"] = pd.to_datetime(df[\"ADMITTIME\"], errors='coerce')\n",
    "df[\"DISCHTIME\"] = pd.to_datetime(df[\"DISCHTIME\"], errors='coerce')\n",
    "df[\"DEATHTIME\"] = pd.to_datetime(df[\"DEATHTIME\"], errors='coerce')\n",
    "\n",
    "# Extract only patients admitted to ED\n",
    "# df_mortality = df[df[\"HOSPITAL_EXPIRE_FLAG\"] == 1].reset_index()\n",
    "df = df[(df.ADMISSION_TYPE.isin(['EMERGENCY'])) & (df.ADMISSION_LOCATION.isin(['EMERGENCY ROOM ADMIT']))]\n",
    "# Display the extracted in-hospital mortality data\n",
    "df.groupby('HOSPITAL_EXPIRE_FLAG').size()\n",
    "df.groupby(['HOSPITAL_EXPIRE_FLAG','ADMISSION_TYPE','ADMISSION_LOCATION']).size()\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f301eb",
   "metadata": {},
   "source": [
    "## Diagnosos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"DIAGNOSES_ICD.csv.gz\"\n",
    "\n",
    "df_diag = pd.read_csv(path, compression='gzip', header=0)\n",
    "print(df_diag.ICD9_CODE.isna().sum())\n",
    "\n",
    "df_diag.dropna(subset=['ICD9_CODE'],axis=0,inplace=True)\n",
    "df_diag['len_code'] = df_diag['ICD9_CODE'].apply(lambda x: len(x))\n",
    "df_diag['ICD9_CODE'] = pd.to_numeric(df_diag['ICD9_CODE'], errors='coerce')\n",
    "df_diag.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ca3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,df_diag, how='left', on=['SUBJECT_ID','HADM_ID'])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8dda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df['ICD9_CODE'][df[\"HOSPITAL_EXPIRE_FLAG\"] == 1].value_counts(normalize=True)*100,3).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9103b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ICD-9 Code\tDescription\tPrevalence (%)\n",
    "518.81\tAcute respiratory failure\t3.291\n",
    "401.9\tUnspecified essential hypertension\t3.117\n",
    "427.31\tAtrial fibrillation\t2.955\n",
    "428.0\tCongestive heart failure, unspecified\t2.897\n",
    "584.9\tAcute kidney failure, unspecified\t2.683\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a07641",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df['ICD9_CODE'][df[\"HOSPITAL_EXPIRE_FLAG\"] == 0].value_counts(normalize=True)*100,3).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ICD-9 Code\tDescription\tPrevalence (%)\n",
    "401.9\tUnspecified essential hypertension\t3.678\n",
    "428.0\tCongestive heart failure, unspecified\t2.564\n",
    "427.31\tAtrial fibrillation\t2.149\n",
    "584.9\tAcute kidney failure, unspecified\t1.959\n",
    "414.01\tCoronary atherosclerosis of native coronary artery\t1.626\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b707c7",
   "metadata": {},
   "source": [
    "## Focus only on Acute kidney failure, unspecified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['ICD9_CODE'] == 5849.0 ].reset_index().drop(columns=[\"index\"])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0cbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('HOSPITAL_EXPIRE_FLAG').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9f49c",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"NOTEEVENTS.csv.gz\"\n",
    "# path = \"/Users/fatemehshah-mohammadi/Documents/Utah_University/Projects/MIMIC-III/NOTEEVENTS.csv.gz\"\n",
    "\n",
    "note = pd.read_csv(path, compression='gzip', header=0)\n",
    "note.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffe51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "note.dropna(subset=['TEXT'],inplace=True)\n",
    "note = note.sort_values(['CHARTDATE','CHARTTIME'], ascending = True)\n",
    "note = note.sort_values('CHARTDATE', ascending = True)\n",
    "note.dropna(subset=['HADM_ID'],inplace=True)\n",
    "\n",
    "note = note.reset_index().drop('index',axis=1)\n",
    "# print(len(note))\n",
    "# print(f\"number of unique patients with note : {note.SUBJECT_ID.nunique()}\")\n",
    "# print('\\n\\n',note.info())\n",
    "# print('\\n\\n',note.isna().sum(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57434ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "note['len_text_1'] = note['TEXT'].apply(lambda x: len(str(x).split(' ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "note['len_text_1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62bbdc",
   "metadata": {},
   "source": [
    "## First day note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a77b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note = note\n",
    "df_note.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689563a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note.dropna(subset=['CHARTDATE','CHARTTIME'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ad791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note['CHARTTIMESTAMP'] = pd.to_datetime(df_note['CHARTDATE'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "df_note['CHARTTIMESTAMP']  = df_note['CHARTTIMESTAMP'] + ' ' +pd.to_datetime(df_note['CHARTTIME'], errors='coerce').dt.strftime('%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfebe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note['CHARTTIMESTAMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 24 h cutoff\n",
    "df_adm['ADMITTIME'] = pd.to_datetime(df_adm.ADMITTIME)\n",
    "df_adm['CUTOFF_24H'] = df_adm.ADMITTIME + pd.Timedelta(hours=24)\n",
    "# Join notes â†’ admissions first\n",
    "n = pd.merge(df_note, df_adm[['SUBJECT_ID','HADM_ID','ADMITTIME','CUTOFF_24H']], \n",
    "                 on=['SUBJECT_ID','HADM_ID'], how='inner')\n",
    "# Filter to the window\n",
    "notes_fd = n[ n.CHARTTIMESTAMP <= n.CUTOFF_24H ]\n",
    "# Aggregate all text in that 24 h window per patient\n",
    "df_note_fd = (notes_fd\n",
    "           .groupby(['SUBJECT_ID','HADM_ID'])['TEXT']\n",
    "           .apply(lambda texts: ' '.join(texts))\n",
    "           .reset_index(name='FIRST_24H_TEXT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92bdd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notes_fd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(notes_fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802809bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_note_fd.head(2)\n",
    "df_note_fd['FIRST_24H_TEXT'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_fd.to_csv('df_before_joining _text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dcd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_fd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_fd = df_note.groupby(['SUBJECT_ID','HADM_ID', 'CHARTDATE'])['TEXT'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "df_note_fd.rename(columns={'TEXT':'FIRST_DAY_TEXT'}, inplace=True)\n",
    "df_note_fd['len_text_2'] = df_note_fd['FIRST_DAY_TEXT'].apply(lambda x: len(str(x).split(' ')))\n",
    "df_note_fd[\"CHARTDATE\"] = pd.to_datetime(df_note_fd[\"CHARTDATE\"], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_fd['len_text_2'] = df_note_fd['FIRST_24H_TEXT'].apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_note_fd))\n",
    "df_note_fd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_fd = df_note_fd.drop_duplicates(['SUBJECT_ID','HADM_ID'],keep='first')\n",
    "print(len(df_note_fd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0eca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_note_fd.len_text_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_note_fd,df, how='inner', on=['SUBJECT_ID','HADM_ID'])\n",
    "df_final['HOSPITAL_EXPIRE_FLAG_CAT']= df_final['HOSPITAL_EXPIRE_FLAG'].apply(lambda x: 'Yes' if x==1 else 'No')\n",
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ada516",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56089c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['len_text_2'].plot.hist(bins=300)\n",
    "df_final['len_text_2'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['len_text_2'][(df_final.len_text_2 < 720) & (df_final.len_text_2 > 1)].plot.hist(bins=300)\n",
    "df_final['len_text_2'][(df_final.len_text_2 < 720) & (df_final.len_text_2 > 1)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[(df_final.len_text_2 < 30000) & (df_final.len_text_2 > 70)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.groupby('HOSPITAL_EXPIRE_FLAG').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_final[df_final['HOSPITAL_EXPIRE_FLAG']==0]))\n",
    "df_ones = df_final[df_final['HOSPITAL_EXPIRE_FLAG'] == 1]\n",
    "\n",
    "# Randomly sample rows with binary_column equal to 0 \n",
    "df_zeros_sampled = df_final[df_final['HOSPITAL_EXPIRE_FLAG'] == 0].sample(850, random_state=42)\n",
    "\n",
    "# Combine the two subsets\n",
    "df_final_sample = pd.concat([df_ones, df_zeros_sampled]).reset_index(drop=True)\n",
    "print(len(df_final_sample[df_final_sample['HOSPITAL_EXPIRE_FLAG']==0]))\n",
    "print(df_final_sample.groupby('HOSPITAL_EXPIRE_FLAG').size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final cohort\n",
    "# df_final_sample.to_excel('df_final_sample.xlsx',engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf48ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_illegal_chars(val):\n",
    "    if isinstance(val, str):\n",
    "        return re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \"\", val)\n",
    "    return val\n",
    "\n",
    "# Apply cleanup to all string values\n",
    "df_final_sample = df_final_sample.applymap(remove_illegal_chars)\n",
    "\n",
    "# Then write to Excel\n",
    "df_final_sample.to_excel('df_final_sample.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1077f2c",
   "metadata": {},
   "source": [
    "## Analysis on the length of the notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6faf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final_sample['len_text_2'][(df_final_sample['HOSPITAL_EXPIRE_FLAG']==0) & (df_final_sample['len_text_2']>7000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final_sample['len_text_2'][(df_final_sample['HOSPITAL_EXPIRE_FLAG']==1) & (df_final_sample['len_text_2']>7000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample['len_text_2'][df_final_sample['HOSPITAL_EXPIRE_FLAG']==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sample['len_text_2'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is df and token lengths are in the 'token_len' column\n",
    "Q1 = df_final_sample['len_text_2'][df_final_sample['HOSPITAL_EXPIRE_FLAG']==0].quantile(0.25)\n",
    "Q3 = df_final_sample['len_text_2'][df_final_sample['HOSPITAL_EXPIRE_FLAG']==0].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "df_filtered_0 = ddf_filtered_0 = df_final_sample[\n",
    "    (df_final_sample['HOSPITAL_EXPIRE_FLAG'] == 0) &\n",
    "    (df_final_sample['len_text_2'] <= upper_bound)\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Removed {len(df_final_sample['len_text_2'][df_final_sample['HOSPITAL_EXPIRE_FLAG']==0]) - len(df_filtered_0)} outlier notes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is df and token lengths are in the 'token_len' column\n",
    "Q1 = df_final_sample['len_text_2'][df_final_sample['HOSPITAL_EXPIRE_FLAG']==1].quantile(0.25)\n",
    "Q3 = df_final_sample['len_text_2'][df_final_sample['HOSPITAL_EXPIRE_FLAG']==1].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers\n",
    "df_filtered_1 = ddf_filtered_0 = df_final_sample[\n",
    "    (df_final_sample['HOSPITAL_EXPIRE_FLAG'] == 1) &\n",
    "    (df_final_sample['len_text_2'] <= upper_bound)\n",
    "]\n",
    "\n",
    "\n",
    "print(f\"Removed {len(df_final_sample['len_text_2'][df_final_sample['HOSPITAL_EXPIRE_FLAG']==1]) - len(df_filtered_1)} outlier notes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.concat([df_filtered_0, df_filtered_1], ignore_index=True)\n",
    "len(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_illegal_chars(val):\n",
    "    if isinstance(val, str):\n",
    "        return re.sub(r\"[\\x00-\\x1F\\x7F-\\x9F]\", \"\", val)\n",
    "    return val\n",
    "\n",
    "# Apply cleanup to all string values\n",
    "df_filtered = df_filtered.applymap(remove_illegal_chars)\n",
    "\n",
    "# Then write to Excel\n",
    "df_filtered.to_excel('df_filtered.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433613c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered.groupby('HOSPITAL_EXPIRE_FLAG').size())\n",
    "print(len(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8347077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('df_filtered.xlsx')\n",
    "print(len(data))\n",
    "print(data.groupby('HOSPITAL_EXPIRE_FLAG_CAT').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8cbc2",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add race and ethnicity\n",
    "path = \"PATIENTS.csv.gz\"\n",
    "df_pt = pd.read_csv(path, compression='gzip', header=0)\n",
    "df_pt.drop_duplicates(inplace=True)\n",
    "df_pt.head(2)\n",
    "data =pd.merge(df_filtered, df_pt[['SUBJECT_ID', 'GENDER', 'DOB', 'DOD']], how='inner', on='SUBJECT_ID')\n",
    "data.drop_duplicates('SUBJECT_ID', inplace=True, keep='first')\n",
    "# data.dropna(subset=['ETHNICITY'],inplace=True)\n",
    "data['ETHNICITY'][data.ETHNICITY.isna()] = 'UNKNOWN (DEFAULT)'\n",
    "data['MARITAL_STATUS'][data.MARITAL_STATUS.isna()] = 'UNKNOWN (DEFAULT)'\n",
    "print(len(data))\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ETHNICITY'][data.ETHNICITY.str.contains('WHITE')]='WHITE'\n",
    "data['ETHNICITY'][data.ETHNICITY.str.contains('ASIAN')] = 'ASIAN'\n",
    "data['ETHNICITY'][data.ETHNICITY.str.contains('HISPANIC')] = 'HISPANIC'\n",
    "data['ETHNICITY'][data.ETHNICITY.str.contains('CARIBBEAN ISLAND')] = 'HISPANIC'\n",
    "data['ETHNICITY'][data.ETHNICITY.str.contains('SOUTH AMERICAN')] = 'HISPANIC'\n",
    "data['ETHNICITY'][data.ETHNICITY.str.contains('PORTUGUESE')] = 'OTHER'\n",
    "data['ETHNICITY'][data.ETHNICITY.str.contains('AMERICAN INDIAN')] = 'AMERICAN INDIAN'\n",
    "data['ETHNICITY'][data.ETHNICITY.str.contains('BLACK')] = 'BLACK'\n",
    "data['ETHNICITY'][data.ETHNICITY.isin(['UNKNOWN/NOT SPECIFIED','UNABLE TO OBTAIN','PATIENT DECLINED TO ANSWER'])]='UNABLE TO OBTAIN'\n",
    "\n",
    "g = data.groupby(['GENDER','HOSPITAL_EXPIRE_FLAG_CAT']).size()\n",
    "print(round(g/g.sum(),2)*100)\n",
    "g = data.groupby(['ETHNICITY','HOSPITAL_EXPIRE_FLAG_CAT']).size()\n",
    "print(round(g/g.sum(),4)*100)\n",
    "g = data.dropna(subset=['MARITAL_STATUS']).groupby(['MARITAL_STATUS','HOSPITAL_EXPIRE_FLAG_CAT']).size()\n",
    "print(round(g/g.sum(),4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENDER distribution\n",
    "g_gender = data.groupby(['GENDER', 'HOSPITAL_EXPIRE_FLAG_CAT']).size()\n",
    "g_gender_pct = round(g_gender / g_gender.sum(), 2) * 100\n",
    "gender_df = g_gender_pct.reset_index(name='Percentage')\n",
    "gender_df['Category'] = 'Gender'\n",
    "\n",
    "# ETHNICITY distribution\n",
    "g_ethnicity = data.groupby(['ETHNICITY', 'HOSPITAL_EXPIRE_FLAG_CAT']).size()\n",
    "g_ethnicity_pct = round(g_ethnicity / g_ethnicity.sum(), 4) * 100\n",
    "ethnicity_df = g_ethnicity_pct.reset_index(name='Percentage')\n",
    "ethnicity_df['Category'] = 'Ethnicity'\n",
    "\n",
    "# MARITAL_STATUS distribution (drop NaNs first)\n",
    "g_marital = data.dropna(subset=['MARITAL_STATUS']).groupby(['MARITAL_STATUS', 'HOSPITAL_EXPIRE_FLAG_CAT']).size()\n",
    "g_marital_pct = round(g_marital / g_marital.sum(), 4) * 100\n",
    "marital_df = g_marital_pct.reset_index(name='Percentage')\n",
    "marital_df['Category'] = 'Marital Status'\n",
    "\n",
    "# Harmonize column names\n",
    "gender_df.rename(columns={'GENDER': 'Group'}, inplace=True)\n",
    "ethnicity_df.rename(columns={'ETHNICITY': 'Group'}, inplace=True)\n",
    "marital_df.rename(columns={'MARITAL_STATUS': 'Group'}, inplace=True)\n",
    "\n",
    "# Concatenate all into one DataFrame\n",
    "final_df = pd.concat([gender_df, ethnicity_df, marital_df], ignore_index=True)\n",
    "\n",
    "# Optional: Sort for readability\n",
    "final_df = final_df[['Category', 'Group', 'HOSPITAL_EXPIRE_FLAG_CAT', 'Percentage']]\n",
    "final_df.sort_values(by=['Category', 'Group'], inplace=True)\n",
    "display(final_df)\n",
    "final_df.to_csv('demograghics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67670fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data\n",
    "pivot_df = final_df.pivot_table(\n",
    "    index='Group',\n",
    "    columns=['Category', 'HOSPITAL_EXPIRE_FLAG_CAT'],\n",
    "    values='Percentage'\n",
    ")\n",
    "\n",
    "# Optional: sort the index and columns for cleaner presentation\n",
    "pivot_df = pivot_df.sort_index().sort_index(axis=1)\n",
    "\n",
    "# Display the pivoted DataFrame\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773647bb",
   "metadata": {},
   "source": [
    "## Please ensure prompting_GPT.py has been executed before running this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4937cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "\n",
    "# Convert string to list\n",
    "def str_to_list(x):\n",
    "\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "def most_comm(x):\n",
    "    return x.most_common(1)[0]\n",
    "\n",
    "def least_common(x):\n",
    "#     print(x.most_common(1)[0])\n",
    "    c = x.most_common(1)[0]\n",
    "#     print(c[1])\n",
    "    if c[0].lower() == 'yes':\n",
    "        return ('No',30-c[1])\n",
    "    else:\n",
    "        return ('Yes',30-c[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a22991",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('df_filtered_gpt_temp_1.xlsx').drop('Unnamed: 0', axis=1)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Convert string to list\n",
    "def str_to_list(x):\n",
    "\n",
    "    return ast.literal_eval(x)\n",
    "\n",
    "\n",
    "def most_comm(lst):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "\n",
    "def least_common(x):\n",
    "#     print(x.most_common(1)[0])\n",
    "    c = x.most_common(1)[0]\n",
    "#     print(c[1])\n",
    "    if c[0].lower() == 'yes':\n",
    "        return ('No',30-c[1])\n",
    "    else:\n",
    "        return ('Yes',30-c[1])\n",
    "    \n",
    "\n",
    "def normalized_yes_probability(lst):\n",
    "    \n",
    "    \n",
    "    top_prediction_and_normalized_prob = []\n",
    "    \n",
    "    \n",
    "    y_n_o_probs_all = []\n",
    "    \n",
    "    for entries in lst:\n",
    "        y_n_o_probs = []\n",
    "        # Convert logprobs to standard probabilities\n",
    "        probs = [(text, math.exp(logprob)) for text, logprob in entries]\n",
    "        \n",
    "        # Total probability for normalization\n",
    "        total_prob = sum(prob for _, prob in probs)\n",
    "        \n",
    "        # Sum of probabilities for entries containing \"yes\" (case-insensitive)\n",
    "        yes_prob = sum(prob for text, prob in probs if 'yes' in text.lower())\n",
    "       \n",
    "        \n",
    "        # Sum of probabilities for entries containing \"no\" (case-insensitive)\n",
    "        no_prob = sum(prob for text, prob in probs if 'no' in text.lower())\n",
    "       \n",
    "        \n",
    "        # Normalized probability\n",
    "        y_n_o_probs.append(no_prob / total_prob if total_prob > 0 else 0.0)\n",
    "        y_n_o_probs.append(yes_prob / total_prob if total_prob > 0 else 0.0)\n",
    "        o_prob = 1- y_n_o_probs[0] - y_n_o_probs[1]\n",
    "        y_n_o_probs.append (o_prob)\n",
    "#         print('yes no other prob: ', y_n_o_probs)\n",
    "        \n",
    "        y_n_o_probs_all.append(y_n_o_probs)\n",
    "       \n",
    "        #print('all:  ', y_n_o_probs_all)\n",
    "#         input()\n",
    "\n",
    "\n",
    "        \n",
    "        top_pred = max(probs, key=lambda x: x[1])\n",
    "        if False:\n",
    "            print('Top 10 logprobs:\\n', entries, '\\n')\n",
    "            print('Top 10 logprobs => probs:\\n', probs, '\\n')\n",
    "            print('total of all 10 probs for normalization:\\n', total_prob, '\\n')\n",
    "            print('yes prob (sum of all containing yes):\\n', yes_prob, '\\n')\n",
    "            print('text + prob (max among those containing yes):\\n', max([x for x in probs if 'yes' in x[0].lower()], key=lambda x: x[1]), '\\n')\n",
    "            print('no prob (sum of all containing no):\\n', no_prob, '\\n')\n",
    "            print('text + prob (max among those containing no):\\n', max([x for x in probs if 'no' in x[0].lower()], key=lambda x: x[1]), '\\n')\n",
    "            print('Response with Max Probability:\\n\\n', top_pred, '\\n')\n",
    "        # normalize prob\n",
    "        top_pred = (top_pred[0], (top_pred[1] / (total_prob if total_prob > 0 else 1)))\n",
    "        top_prediction_and_normalized_prob.append(top_pred)\n",
    "\n",
    "        \n",
    "        \n",
    "    return y_n_o_probs_all\n",
    "    # return y_n_o_probs_all, top_prediction_and_normalized_prob\n",
    "\n",
    "\n",
    "def sample_final_prediction(lst):\n",
    "    l=[]\n",
    "    for item in lst:\n",
    "        l.append(item.index(max(item)))\n",
    "#     print(l)\n",
    "    return l   \n",
    "\n",
    "def true_class_prob (x):\n",
    "    return [sublist[x[1]] for sublist in x[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.groupby('HOSPITAL_EXPIRE_FLAG').size())\n",
    "print(len(data['prediction_logprob'].apply(str_to_list).iloc[0]), len(data['prediction_logprob'].apply(str_to_list).iloc[0][0]))\n",
    "\n",
    "data['prediction_logprob'] = data['prediction_logprob'].apply(str_to_list)\n",
    "data ['y_n_o_probs_all'] = data['prediction_logprob'].apply(normalized_yes_probability)\n",
    "# sample_final_prediction(data ['y_n_o_probs_all'][0])\n",
    "data ['sample_final_prediction'] = data ['y_n_o_probs_all'].apply(sample_final_prediction)\n",
    "data ['final_prediction'] = data ['sample_final_prediction'].apply(most_comm)\n",
    "data['HOSPITAL_EXPIRE_FLAG_CAT']= data['HOSPITAL_EXPIRE_FLAG'].apply(lambda x: 'Yes' if x==1 else 'No')\n",
    "data['true_class_prob'] = data[['y_n_o_probs_all','HOSPITAL_EXPIRE_FLAG']].apply(true_class_prob,axis=1)\n",
    "data['nonconformity_scores'] = data['true_class_prob'].apply(lambda lst: [1-p for p in lst])\n",
    "all_nonconformity_scores = sum(data['nonconformity_scores'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7afb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a calibration set and a test set (e.g., 50% each)\n",
    "cal = data.sample(frac=0.5, random_state=42)\n",
    "test = data.drop(cal.index).reset_index(drop=True)\n",
    "\n",
    "## save caliberation and test dataset\n",
    "# cal.to_csv('calibration_gpt_temp_1_v2.csv')\n",
    "# test.to_excel('test_gpt_temp_1_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1664e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = round(accuracy_score(data['HOSPITAL_EXPIRE_FLAG'], data['final_prediction'])*100,2)\n",
    "precision = round(precision_score(data['HOSPITAL_EXPIRE_FLAG'], data['final_prediction'])*100,2)\n",
    "recall = round(recall_score(data['HOSPITAL_EXPIRE_FLAG'], data['final_prediction'])*100,2)\n",
    "f1 = round(f1_score(data['HOSPITAL_EXPIRE_FLAG'], data['final_prediction'])*100,2)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bfd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71648138",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29475c33",
   "metadata": {},
   "source": [
    "## Accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdeada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate_rows = data[data['HOSPITAL_EXPIRE_FLAG'] == data['final_prediction']]\n",
    "accurate_rows.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(accurate_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b288402",
   "metadata": {},
   "source": [
    "## Non Accurate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3562c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_accurate_rows = data[data['HOSPITAL_EXPIRE_FLAG'] != data['final_prediction']]\n",
    "non_accurate_rows.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ad9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accurate_rows['len_text_2'].hist(bins= 20)\n",
    "plt.xlabel('number of tokens per note')\n",
    "plt.ylabel('frequency')\n",
    "accurate_rows['len_text_2'].describe().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for the first set\n",
    "plt.hist(accurate_rows['len_text_2'], bins=50, color='blue', alpha=0.5, label='Accuate prediction')\n",
    "\n",
    "# Plot a second histogram (for example, another column or subset) with a different color\n",
    "# Here, we assume you're overlaying another histogram from the same DataFrame.\n",
    "# Replace 'other_column' with your actual column or subset if needed.\n",
    "plt.hist(non_accurate_rows['len_text_2'], bins=50, color='red', alpha=0.5, label='Non-Accuate prediction')\n",
    "\n",
    "plt.xlabel('Number of tokens per note')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for the first set\n",
    "plt.figure()\n",
    "plt.hist(accurate_rows['len_text_2'][accurate_rows['HOSPITAL_EXPIRE_FLAG']==1], bins=50, color='blue', alpha=0.5, label='Accuate prediction')\n",
    "plt.xlabel('Number of tokens per note')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Accurate in hospital mortality (flag=1)')\n",
    "plt.show()\n",
    "\n",
    "# Second histogram in a separate window\n",
    "plt.figure()\n",
    "plt.hist(non_accurate_rows['len_text_2'][non_accurate_rows['HOSPITAL_EXPIRE_FLAG']==1], bins=50, color='red', alpha=0.5, label='Non-Accuate prediction')\n",
    "\n",
    "plt.xlabel('Number of tokens per note')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Non-Accurate in hospital mortality (flag=1)')\n",
    "plt.show()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(accurate_rows['len_text_2'][accurate_rows['HOSPITAL_EXPIRE_FLAG']==1].describe().round())\n",
    "print(non_accurate_rows['len_text_2'][non_accurate_rows['HOSPITAL_EXPIRE_FLAG']==1].describe().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for the first set\n",
    "plt.figure()\n",
    "plt.hist(accurate_rows['len_text_2'][accurate_rows['HOSPITAL_EXPIRE_FLAG']==0], bins=50, color='blue', alpha=0.5, label='Accuate prediction')\n",
    "plt.xlabel('Number of tokens per note')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Accurate in hospital mortality (flag=0)')\n",
    "plt.show()\n",
    "\n",
    "# Second histogram in a separate window\n",
    "plt.figure()\n",
    "plt.hist(non_accurate_rows['len_text_2'][non_accurate_rows['HOSPITAL_EXPIRE_FLAG']==0], bins=50, color='red', alpha=0.5, label='Non-Accuate prediction')\n",
    "\n",
    "plt.xlabel('Number of tokens per note')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Non-Accurate in hospital mortality (flag=0)')\n",
    "plt.show()\n",
    "plt.show()\n",
    "\n",
    "print('accurate')\n",
    "print(accurate_rows['len_text_2'][accurate_rows['HOSPITAL_EXPIRE_FLAG']==0].describe().round())\n",
    "\n",
    "print('non-accurate')\n",
    "print(non_accurate_rows['len_text_2'][non_accurate_rows['HOSPITAL_EXPIRE_FLAG']==0].describe().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_flag_1 = data[data['HOSPITAL_EXPIRE_FLAG'] == 1]\n",
    "mortality_flag_1.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166624c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality = mortality_flag_1[['len_text_2','HOSPITAL_EXPIRE_FLAG','final_prediction']]\n",
    "mortality.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468b36a",
   "metadata": {},
   "source": [
    "## t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a40c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66271031",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = mortality[mortality['final_prediction'] == 1]['len_text_2']\n",
    "incorrect = mortality[mortality['final_prediction'] == 0]['len_text_2']\n",
    "\n",
    "# Perform an independent samples t-test\n",
    "t_stat, p_value = ttest_ind(correct, incorrect)\n",
    "\n",
    "print(\"T-test results:\")\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f44246",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_flag_0 = data[data['HOSPITAL_EXPIRE_FLAG'] == 0]\n",
    "mortality_flag_0.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e586f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_mortality = mortality_flag_0[['len_text_2','HOSPITAL_EXPIRE_FLAG','final_prediction']]\n",
    "correct = non_mortality[non_mortality['final_prediction'] == 0]['len_text_2']\n",
    "incorrect = non_mortality[non_mortality['final_prediction'] == 1]['len_text_2']\n",
    "\n",
    "# Perform an independent samples t-test\n",
    "t_stat, p_value = ttest_ind(correct, incorrect)\n",
    "\n",
    "print(\"T-test results:\")\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_flag_1 = data[data['HOSPITAL_EXPIRE_FLAG'] == 1]\n",
    "mortality_flag_1.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663bd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_flag_1_100 = mortality_flag_1[mortality_flag_1['len_text_2']<=100]\n",
    "mortality_flag_1_100.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_flag_1_1 = mortality_flag_1[mortality_flag_1['final_prediction']==1]\n",
    "mortality_flag_1_1.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dac77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_flag_1_0 = mortality_flag_1[mortality_flag_1['final_prediction']==0]\n",
    "mortality_flag_1_0.groupby(['HOSPITAL_EXPIRE_FLAG','final_prediction']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b7ae6",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "print(type(stops))\n",
    "# stop = text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "stops.update('pt clinic work without with vs within'.split())\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "## Functions\n",
    "def stemmed_words(doc):\n",
    "    return ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(doc)])\n",
    "\n",
    "def remove_punctuation(doc):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ' '.join([word for word in nltk.word_tokenize(doc) if word not in exclude])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(mortality_flag_1_1['FIRST_DAY_TEXT'].astype(str).tolist())\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming function\n",
    "def stemmed_words(doc):\n",
    "    return ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(doc)])\n",
    "\n",
    "# Apply stemming to the documents\n",
    "stemmed_documents = [stemmed_words(doc) for doc in mortality_flag_1_1['FIRST_DAY_TEXT']]\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,2),lowercase=True,stop_words=set(stops))\n",
    "matrix = vect.fit_transform(stemmed_documents)\n",
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "\n",
    "# Create a word cloud from frequencies\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vect.vocabulary_)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Most Frequent Words Larger, Less Frequent Words Smaller\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming function\n",
    "def stemmed_words(doc):\n",
    "    return ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(doc)])\n",
    "\n",
    "# Apply stemming to the documents\n",
    "stemmed_documents = [stemmed_words(doc) for doc in mortality_flag_1_0['FIRST_DAY_TEXT']]\n",
    "\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,2),lowercase=True,stop_words=set(stops))\n",
    "matrix = vect.fit_transform(stemmed_documents)\n",
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "\n",
    "# Create a word cloud from frequencies\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vect.vocabulary_)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud: Most Frequent Words Larger, Less Frequent Words Smaller\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cloud(doc):\n",
    "\n",
    "    # Remove punctuation\n",
    "    doc = remove_punctuation(doc)\n",
    "    \n",
    "    # Apply stemming to the documents\n",
    "    doc = stemmed_words(doc)\n",
    "\n",
    "\n",
    "    vect = TfidfVectorizer(ngram_range=(1,2),lowercase=True,stop_words=set(stops))\n",
    "    matrix = vect.fit_transform(doc)\n",
    "    feature_names = vect.get_feature_names()\n",
    "\n",
    "\n",
    "    # Create a word cloud from frequencies\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vect.vocabulary_)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud: Most Frequent Words Larger, Less Frequent Words Smaller\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
